{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Toolkit for Splunk - Notebook for TensorFlow 2.0\n",
    "This notebook is based on the **autoencoder** notebook of the standard DSDL containers, and has been modified for educational purposes.\n",
    "The original code can be found in: [Splunk MLTK Container Docker Github](https://github.com/splunk/splunk-mltk-container-docker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder Example\n",
    "This notebook contains an example workflow how to work on custom containerized code that seamlessly interfaces with the Deep Learning Toolkit for Splunk. As an example we use a custom autoencoder built on keras and tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries \n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 12:54:42.000038: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-21 12:54:42.112735: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-21 12:54:42.117781: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-21 12:54:42.117795: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-21 12:54:42.140911: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-21 12:54:42.719107: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 12:54:42.719158: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 12:54:42.719164: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# mltkc_import\n",
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.22.1\n",
      "pandas version: 1.5.1\n",
      "TensorFlow version: 2.10.0\n",
      "Keras version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)\n",
    "print(\"TensorFlow version: \" + tf.__version__)\n",
    "print(\"Keras version: \" + keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a prepared dataset into this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| inputlookup iris.csv <br>| fit MLTKContainer algo=autoencoder epochs=100 batch_size=4 components=2 petal_length petal_width sepal_length sepal_width into app:iris_autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"my_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# mltkc_stage\n",
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bytes_received  bytes_sent  packets_received  packets_sent  \\\n",
      "0             170          85                 1             1   \n",
      "\n",
      "   used_by_malware_yes  used_by_malware_no  \n",
      "0                    1                   0  \n",
      "(69510, 6)\n",
      "{'options': {'params': {'mode': 'stage', 'algo': 'SideChannel_model'}, 'args': ['bytes_received', 'bytes_sent', 'packets_received', 'packets_sent', 'used_by_malware_yes', 'used_by_malware_no'], 'feature_variables': ['bytes_received', 'bytes_sent', 'packets_received', 'packets_sent', 'used_by_malware_yes', 'used_by_malware_no'], 'model_name': 'SC_Autoencoder', 'algo_name': 'MLTKContainer', 'mlspl_limits': {'handle_new_cat': 'default', 'max_distinct_cat_values': '100', 'max_distinct_cat_values_for_classifiers': '100', 'max_distinct_cat_values_for_scoring': '100', 'max_fit_time': '600', 'max_inputs': '100000', 'max_memory_usage_mb': '4000', 'max_model_size_mb': '30', 'max_score_time': '600', 'use_sampling': 'true'}, 'kfold_cv': None}, 'feature_variables': ['bytes_received', 'bytes_sent', 'packets_received', 'packets_sent', 'used_by_malware_yes', 'used_by_malware_no']}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "df, param = stage(\"SC_Autoencoder\")\n",
    "print(df[0:1])\n",
    "print(df.shape)\n",
    "print(str(param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# mltkc_init\n",
    "# initialize the model\n",
    "# params: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    X = df[param['feature_variables']]\n",
    "    print(\"FIT build model with input shape \" + str(X.shape))\n",
    "    components = 4\n",
    "    components_2 = 2\n",
    "    activation_fn = 'relu'\n",
    "    # learning_rate = 0.001\n",
    "    # epsilon=0.00001 # default 1e-07\n",
    "    if 'options' in param:\n",
    "        if 'params' in param['options']:\n",
    "            if 'components' in param['options']['params']:\n",
    "                components = int(param['options']['params']['components'])\n",
    "            if 'activation_func' in param['options']['params']:\n",
    "                activation_fn = param['options']['params']['activation_func']\n",
    "            if 'components_2' in param['options']['params']:\n",
    "                components_2 = int(param['options']['params']['components_2'])\n",
    "                \n",
    "    input_shape = int(X.shape[1])\n",
    "    encoder_layer = keras.layers.Dense(components, input_dim=input_shape, activation=activation_fn, kernel_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None), bias_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None))\n",
    "    decoder_layer = keras.layers.Dense(input_shape, activation=activation_fn, kernel_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None), bias_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None))\n",
    "    # new layer\n",
    "    extra_layer = keras.layers.Dense(components_2, activation=activation_fn)\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(encoder_layer)\n",
    "    model.add(extra_layer) # add new layer\n",
    "    model.add(decoder_layer)\n",
    "    #opt = keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT build model with input shape (69510, 6)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 4)                 28        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56\n",
      "Trainable params: 56\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 12:54:59.838936: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-21 12:54:59.838963: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-21 12:54:59.838982: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a95066599963): /proc/driver/nvidia/version does not exist\n",
      "2023-03-21 12:54:59.839561: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# test mltkc_stage_create_model\n",
    "model = init(df,param)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# mltkc_stage_create_model_fit\n",
    "# returns a fit info json object\n",
    "def fit(model,df,param):\n",
    "    returns = {}\n",
    "    X = df[param['feature_variables']]\n",
    "    model_epochs = 20\n",
    "    model_batch_size = 32\n",
    "    if 'options' in param:\n",
    "        if 'params' in param['options']:\n",
    "            if 'epochs' in param['options']['params']:\n",
    "                model_epochs = int(param['options']['params']['epochs'])\n",
    "            if 'batch_size' in param['options']['params']:\n",
    "                model_batch_size = int(param['options']['params']['batch_size'])\n",
    "    # connect model training to tensorboard\n",
    "    log_dir=\"/srv/notebooks/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    # run the training\n",
    "    returns['fit_history'] = model.fit(x=X,\n",
    "                                       y=X, \n",
    "                                       verbose=2, \n",
    "                                       epochs=model_epochs, \n",
    "                                       batch_size=model_batch_size, \n",
    "                                       #validation_data=(X, Y),\n",
    "                                       callbacks=[tensorboard_callback])\n",
    "    # memorize parameters\n",
    "    returns['model_epochs'] = model_epochs\n",
    "    returns['model_batch_size'] = model_batch_size\n",
    "    returns['model_loss_acc'] = model.evaluate(x = X, y = X)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2173/2173 - 2s - loss: 21052524.0000 - accuracy: 0.4984 - 2s/epoch - 888us/step\n",
      "Epoch 2/20\n",
      "2173/2173 - 2s - loss: 1061591.6250 - accuracy: 0.7079 - 2s/epoch - 699us/step\n",
      "Epoch 3/20\n",
      "2173/2173 - 2s - loss: 765186.6875 - accuracy: 0.7080 - 2s/epoch - 704us/step\n",
      "Epoch 4/20\n",
      "2173/2173 - 2s - loss: 764367.5000 - accuracy: 0.7443 - 2s/epoch - 857us/step\n",
      "Epoch 5/20\n",
      "2173/2173 - 2s - loss: 764579.0000 - accuracy: 0.7845 - 2s/epoch - 813us/step\n",
      "Epoch 6/20\n",
      "2173/2173 - 2s - loss: 763586.6250 - accuracy: 0.8067 - 2s/epoch - 784us/step\n",
      "Epoch 7/20\n",
      "2173/2173 - 2s - loss: 765639.6875 - accuracy: 0.8116 - 2s/epoch - 845us/step\n",
      "Epoch 8/20\n",
      "2173/2173 - 2s - loss: 770080.6875 - accuracy: 0.8142 - 2s/epoch - 883us/step\n",
      "Epoch 9/20\n",
      "2173/2173 - 2s - loss: 761182.3125 - accuracy: 0.8177 - 2s/epoch - 867us/step\n",
      "Epoch 10/20\n",
      "2173/2173 - 2s - loss: 761846.0000 - accuracy: 0.8191 - 2s/epoch - 818us/step\n",
      "Epoch 11/20\n",
      "2173/2173 - 2s - loss: 761949.0000 - accuracy: 0.8220 - 2s/epoch - 728us/step\n",
      "Epoch 12/20\n",
      "2173/2173 - 2s - loss: 766251.1250 - accuracy: 0.8260 - 2s/epoch - 890us/step\n",
      "Epoch 13/20\n",
      "2173/2173 - 2s - loss: 760589.8125 - accuracy: 0.8294 - 2s/epoch - 791us/step\n",
      "Epoch 14/20\n",
      "2173/2173 - 2s - loss: 761622.5625 - accuracy: 0.8326 - 2s/epoch - 728us/step\n",
      "Epoch 15/20\n",
      "2173/2173 - 2s - loss: 765864.4375 - accuracy: 0.8319 - 2s/epoch - 739us/step\n",
      "Epoch 16/20\n",
      "2173/2173 - 2s - loss: 758104.3125 - accuracy: 0.8354 - 2s/epoch - 711us/step\n",
      "Epoch 17/20\n",
      "2173/2173 - 2s - loss: 762101.7500 - accuracy: 0.8367 - 2s/epoch - 708us/step\n",
      "Epoch 18/20\n",
      "2173/2173 - 1s - loss: 758052.3750 - accuracy: 0.8370 - 1s/epoch - 690us/step\n",
      "Epoch 19/20\n",
      "2173/2173 - 1s - loss: 760049.0000 - accuracy: 0.8367 - 1s/epoch - 670us/step\n",
      "Epoch 20/20\n",
      "2173/2173 - 2s - loss: 758155.5000 - accuracy: 0.8374 - 2s/epoch - 693us/step\n",
      "2173/2173 [==============================] - 2s 804us/step - loss: 754323.0000 - accuracy: 0.8374\n",
      "[754323.0, 0.8374334573745728]\n"
     ]
    }
   ],
   "source": [
    "returns = fit(model,df,param)\n",
    "print(returns['model_loss_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "# mltkc_stage_create_model_apply\n",
    "def apply(model,df,param):\n",
    "    X = df[param['feature_variables']]\n",
    "    reconstruction = model.predict(x = X)\n",
    "#     intermediate_layer_model = keras.Model(inputs=model.input, outputs=model.layers[0].output)\n",
    "#     hidden = intermediate_layer_model.predict(x = X)\n",
    "#     y_hat = pd.concat([pd.DataFrame(reconstruction).add_prefix(\"reconstruction_\"), pd.DataFrame(hidden).add_prefix(\"hidden_\")], axis=1)\n",
    "\n",
    "    reconstruction_error = tf.keras.losses.mean_squared_error(X, reconstruction).numpy()\n",
    "    y_hat = pd.concat([pd.DataFrame(reconstruction_error).add_prefix(\"reconstructionError_\")], axis=1)\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2173/2173 [==============================] - 2s 735us/step\n",
      "       reconstructionError_0\n",
      "0                 241.595108\n",
      "1                 159.410263\n",
      "2                 169.295898\n",
      "3              441326.625000\n",
      "4              135521.484375\n",
      "...                      ...\n",
      "69505           73840.789062\n",
      "69506           32123.349609\n",
      "69507           42464.121094\n",
      "69508           42464.121094\n",
      "69509           42464.121094\n",
      "\n",
      "[69510 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# test mltkc_stage_create_model_apply\n",
    "y_hat = apply(model,df,param)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model,name):\n",
    "    # save keras model to hdf5 file\n",
    "    # https://www.tensorflow.org/beta/tutorials/keras/save_and_restore_models\n",
    "    model.save(MODEL_DIRECTORY + name + \".h5\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(name):\n",
    "    model = keras.models.load_model(MODEL_DIRECTORY + name + \".h5\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"tensorflow\": tf.__version__, \"keras\": keras.__version__} }\n",
    "    if model is not None:\n",
    "        # Save keras model summary to string:\n",
    "        s = []\n",
    "        model.summary(print_fn=lambda x: s.append(x+'\\n'))\n",
    "        returns[\"summary\"] = ''.join(s)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'version': {'tensorflow': '2.10.0', 'keras': '2.10.0'},\n",
       " 'summary': 'Model: \"sequential\"\\n_________________________________________________________________\\n Layer (type)                Output Shape              Param #   \\n=================================================================\\n dense (Dense)               (None, 4)                 28        \\n                                                                 \\n dense_2 (Dense)             (None, 2)                 10        \\n                                                                 \\n dense_1 (Dense)             (None, 6)                 18        \\n                                                                 \\n=================================================================\\nTotal params: 56\\nTrainable params: 56\\nNon-trainable params: 0\\n_________________________________________________________________\\n'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
